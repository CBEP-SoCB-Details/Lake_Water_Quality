---
title: "Prelimnary Analysis of Lakes SECCHI Depth Data"
output: html_notebook
---
# Load Libraries
```{r}
library(tidyverse)
library(readxl)
library(mblm)
citation(package='mblm')
```

# Read Data
Here we read in the data, do a lot of renaming, and convert some to factors, and finally, add a Year term.  Note the filter removing NAs is because one lake is included in these data but has no actual data -- only NAs for the Secchi Depth.  Also note that the coding for the "Secchi on Bottom" flag is inconsistent, with four possible codes: "Y", "B", "N", and "".  I'm interpreting the first two as evidence that the Secchi Disk was on the botton, "N" as evidence that it was not, and "" as a missing value.  This needsclarification.

```{r}
sibfldnm <- 'Derived Data'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'Secchi.xlsx'

recentsecchi.data <- read_excel(paste(sibling, fn, sep='/'), col_types = c("skip", 
    "numeric", "text", "numeric", "date", 
    "text", "numeric", "text", "text", "numeric", 
    "numeric", "text")) %>%
  rename_at(2:11, ~paste(substr(.,1,1), tolower(substr(.,2,nchar(.))), sep='')) %>%  # Change capitalization
  rename(Secchi = `Secchi depth`) %>%
  filter(! is.na(Secchi)) %>%
  mutate(MIDAS = factor(MIDAS)) %>%
  
  rename(CensoredFlag = `Secchi on bottom?`) %>%
  mutate(CensoredFlag = fct_recode(CensoredFlag, 'Y'='B')) %>%
  mutate(CensoredFlag = CensoredFlag == 'Y') %>%
           
  mutate(Scope = factor(Scope, levels = as.character(1:5),
                        labels = c('None', 'Plain', 'Slant', 'Slant with Mask', 'Flat with Mask'))) %>%
  rename(Wind = `Wind level`) %>%
  rename(WindDir = `Wind direction` ) %>%
  mutate(WindDir = factor (WindDir, levels = 1:8, labels = c('N', 'NE', 'E', 'SE', 'S', 'Sw', 'W', 'NW'))) %>%
  rename(CloudCover = `Cloud cover`) %>%
  mutate(CloudCover = factor (CloudCover, levels = c('B', 'C', 'O'),
                              labels = c('Clear', 'Cloudy Bright' , 'Heavy Overcast'))) %>%
 
  mutate(Year = as.numeric(format(Date, format = '%Y'))) %>%
  mutate(Month = as.numeric(format(Date, format = '%m'))) %>%
  filter(Year > 2008)
```

# Read Morphometric Data
Read in morphometric data, and filter to lakes for which we have at lest some Secchi data.
```{r}
fn <- 'CB_Lakes_Morphometry Metric.xlsx'
morpho.data <- read_excel(paste(sibling, fn, sep='/')) %>% 
  filter(MIDAS %in% levels(factor(recentsecchi.data$MIDAS))) %>%
  mutate(MIDAS = factor(MIDAS)) %>%
  rename(Town = `Town(s)`) %>%
  rename(Flushing = `Flushing Rate (times/yr)`) %>%
  rename(TrophicCategory = `Trophic Category`) %>%
  mutate(Drainage_M2 = as.numeric(Drainage_M2)) %>%
  mutate(Flushing = as.numeric(Flushing))
rm(fn,parent,sibfldnm,sibling)
```

# Select Lakes With Sufficient Data for Trends
We want to select only those lakes with a "reasonable" amount of data for extrapolating trends.  We could soften this constraint using mixed models, but it appears both statistically unwise and potentially difficult to expain to audiences.

For the following analyses, we emphasise lakes with data from the last ten years.
```{r}
recenttotals <- recentsecchi.data %>%
  group_by(MIDAS) %>%
  summarize(Total = sum(! is.na(Secchi)))

recentsampled <- recentsecchi.data %>%
  group_by(MIDAS, Year) %>%
  summarize(Sampled = sum(! is.na(Secchi))>0) %>%
  mutate(MIDAS2 = factor(MIDAS, levels = unique(recenttotals$MIDAS[order(recenttotals$Total)]))) %>%
  mutate(Name = morpho.data$Name[match(MIDAS, morpho.data$MIDAS)]) %>%
  group_by(MIDAS) %>%
  summarize(recentyears = sum(Sampled[Year>2008], na.rm=TRUE))

RecentLakesMIDAS <- recentsampled$MIDAS[recentsampled$recentyears>=5]

length(RecentLakesMIDAS)
rm(recenttotals)
```

# Analysis
We consiodered four distinct strategies for analysis here:
(1) Application of Thiel-Sen estimators and KEnall's Tau lake by lake
(2) Analysis of medians for each lake, using ordinary least squares
(3) Analysis of medians for each lake, using weighted least squares to account for changes in sample size
(4) Multi-level linear models

## (1) Lake by Lake Trends
The following is based on code originally used in preparing the 2015 State of the Bay Report. The use of a for loop makes this potentially rather inefficient.  A more modern approach would use tidyverse function s group_by() and summarize(), but I could not get the Thiel-Sen estimator code (functions mblm() and cor.test() ) to run inside summarize.

Note that in the following I calculate both Thiel-Sen and linear model slopes.  The Thiel-Sen estimator (actually the modification of that estimator due to Siegel, acording to the help files) is resistant to outliers and other violations of the assumption of normality.  It is thus better suited to this (unsupervized) application.  I calculated linear model slopes to show that the qualitative results of my analysis are largely insensitive to the details of the analysis used.

Significance of the Thiel-Sel slopes is based on Kendall's Tau, which is closely related to the Thiel-Sen estimator, not the Wilcoxon.Test used in the default summary.mblm() function. 

Kendall's Tau  tests for a monotonic relationship (corelation between orderings) between two series.  Here we have a small problem, because there are many ties in the lake Secchi data because observatiosn are truncated, often to 10cm intervals.

I have found conflicting informaiton on how cor.test calculates Tau, but my conclusiopn is it is correctly clculating "Tau-B" when necessary, and can handle tied values, but prints and error to indicate that p values are estimated, not exxactly calculated.

A search for Kendall's Tau-B online suggests the statistic is also available in packages Kendall and DescTools.

Kendall computes a ties-corrected significance level that should be adequate for moderate sample sizes.  The help file does not use the Tau-A, Tau-B and Tau-C terminology, but it purports to estimate P values apprpriately.  I think this may be effectively identical to cor.test() method for our purposes. (The package handles so mother types of analyses, liek seasonal analyses) .

DescTools offers direct calculation of the Kendall's Tau A and Kendall's Tau B values, along with confidence intervals, but not P values.  DescTools suggests the cor.test(method='kendall") version does calculates Tau-B, but I can not confirm that elsewhere.  The 

Some comments: This is ignoring seasonal patterns
This is not using the more robust Teil-Sen estimators
This is ignoring right censored data, which is thankfully relatively uncommon.


```{r}
recentresults <- recentsecchi.data %>%
  filter(MIDAS %in% RecentLakesMIDAS) %>%
  group_by(MIDAS) %>%
  summarize (
    Lakes = first(MIDAS),
    Samples = n(),
	  Stations = length(levels(factor(Station))),
    Years = length(levels(factor(Year))),
    FirstYears = min(Year),
  	LastYears = max(Year),
    Means = mean(Secchi, na.rm=TRUE),
    Medians = median(Secchi, na.rm=TRUE),
    Slopes = lm(Secchi~ Year)$coef[2],
    PValues = anova(lm(Secchi~ Year))[1,5],
    TSPValues = cor.test(Secchi,Year)$p.value
  )
 
# Now add the Thiel-Sen slope estimate.  I could not figure out how to run this inside the summarize() function.  I included the cor,test call here just to isolate teh robust statistics together. 

TSSlopes <- vector(mode = "numeric", length = 0)
TSPValues <- vector(mode = "numeric", length = 0)
for (lake in RecentLakesMIDAS)
  {  
    thisdata <- subset(recentsecchi.data, recentsecchi.data$MIDAS == lake)
    TStest <- mblm(Secchi ~ Year, dataframe = thisdata)
    TSSlopes <- c(TSSlopes, TStest$coef[2])
    TSPValues = c(TSPValues, cor.test(~ Secchi + Year, data = thisdata)$p.value)
}

recentresults$TSSlopes <- TSSlopes[match(RecentLakesMIDAS, recentresults$Lakes)]
recentresults$TSPValues <- TSPValues[match(RecentLakesMIDAS, recentresults$Lakes)]

write.table(recentresults, "RecentLakeSlopes.csv", sep = ",")
```
      
### Comparison of Linear and Thiel-Sen Estimator Slopes
There is a high corelation between the two slope estimators, as one might expect.
```{r}
plt <-  ggplot(recentresults, aes(Slopes, TSSlopes))  + geom_point() + geom_smooth(method = 'lm')
plt
```
What's with the outlier?  Why are teh two slope estimates so different for that one lake?
```{r}
recentresults %>%
  mutate(Names = morpho.data$Name[match(Lakes, morpho.data$MIDAS)]) %>%
  select(Slopes, TSSlopes, Names) %>%
  ggplot(aes(Slopes, TSSlopes)) +
  #geom_point() +
  geom_text(aes(label = Names))
```
So the outlier is Sebago Lake, MIDAS = 5786. Although I know there is a lot of recent Sebago Lake Data from tehPortland Water District, that data is not fully reported in this DEP-derived data set.  Ther eare only 22 secchi depth observations of Sebago Lake from the last ten years in this data set.

```{r}
morpho.data$Name[morpho.data$MIDAS == 5786]
length(recentsecchi.data$MIDAS[recentsecchi.data$MIDAS==3390])
```
Lets look at the data record for Sebago Lake.
```{r}
tmp <- recentsecchi.data %>% 
  filter(MIDAS == 5786)

ggplot(tmp, aes(Year, Secchi)) + geom_point(aes(color = CensoredFlag), alpha = 0.5)

```
So, what we see here is the impact of two outliers -- both due to censored data from a previously unsampled location - on the estimate of slope based on a classic liear model. These are real data, but bias any analysis, and should probably be omitted from analysis.  The Theil-Sen estimator is RELATIVELY robust to that kind of a problem, which is precisely why we use it. The alternative is to drop all censored data, or alternatively, drop Station 50 from these data.

### Results if LM used
We filter out near-zero slopes (< 0.005m per year, or ~ 5 cm per decade) as meaningless.  We would not normally d othat, but the Thiel-Sen slopes produces  couple of lakes with slopes exactly equal to zero, which nevertheless has statistically significant tresnts by Kendall's tau.
```{r}
recentresults %>%
  mutate(significant = PValues<=0.05 & ! abs(Slopes)<0.005 ) %>%
  mutate(category = ifelse(Slopes<0, "Declining",  ifelse(abs(Slopes)<0.002, 'Zero', 'Increasing'))) %>%
  group_by(category, significant)%>%
  summarize(n=n()) %>%
  spread(category, n)
```

```{r}
declininglakes <- recentresults$Lakes[recentresults$PValues<0.05 & recentresults$Slopes<=-0.005]
length(declininglakes)
morpho.data$Name[match(declininglakes,morpho.data$MIDAS)]
```


### Results if Thiel-Sen USed
```{r}
recentresults %>%
  mutate(significant = TSPValues<=0.05  & ! abs(Slopes) < 0.005 ) %>%
  mutate(category = ifelse(TSSlopes<0, "Declining",  ifelse(TSSlopes<0.002, 'Zero', 'Increasing'))) %>%
  group_by(category, significant)%>%
  summarize(n=n()) %>%
  spread(category, n)
```

```{r}
declininglakes <- recentresults$Lakes[recentresults$TSPValues<0.05 & recentresults$TSSlopes<=-0.005]
length(declininglakes)
morpho.data$Name[match(declininglakes,morpho.data$MIDAS)]
```